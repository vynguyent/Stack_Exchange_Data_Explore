{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.0.0\n",
      "      /_/\n",
      "\n",
      "Using Python version 2.7.12 (default, Jun 29 2016 11:07:13)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "spark_home = \"D:\\spark-2.0.0-bin-hadoop2.7\"\n",
    "os.environ['SPARK_HOME'] = spark_home\n",
    "sys.path.append(os.path.join(spark_home, \"python\"))\n",
    "sys.path.append(os.path.join(spark_home, \"python\", \"lib\", \"py4j-0.10.1-src.zip\"))\n",
    "\n",
    "execfile(os.path.join(spark_home, \"python\", \"pyspark\", \"shell.py\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xmltodict\n",
    "import json\n",
    "communities = ['3dprinting.stackexchange.com', 'academia.stackexchange.com', 'ai.stackexchange.com', 'android.stackexchange.com', 'anime.stackexchange.com', 'apple.stackexchange.com', 'arabic.stackexchange.com', 'arduino.stackexchange.com', 'askubuntu.com', 'astronomy.stackexchange.com', 'aviation.stackexchange.com', 'avp.stackexchange.com', 'beer.stackexchange.com', 'bicycles.stackexchange.com', 'biology.stackexchange.com', 'bitcoin.stackexchange.com', 'blender.stackexchange.com', 'boardgames.stackexchange.com', 'bricks.stackexchange.com', 'buddhism.stackexchange.com', 'chemistry.stackexchange.com', 'chess.stackexchange.com', 'chinese.stackexchange.com', 'christianity.stackexchange.com', 'civicrm.stackexchange.com', 'codegolf.stackexchange.com', 'codereview.stackexchange.com', 'coffee.stackexchange.com', 'cogsci.stackexchange.com', 'computergraphics.stackexchange.com', 'cooking.stackexchange.com', 'craftcms.stackexchange.com', 'crafts.stackexchange.com', 'crypto.stackexchange.com', 'cs.stackexchange.com', 'cstheory.stackexchange.com', 'datascience.stackexchange.com', 'dba.stackexchange.com', 'diy.stackexchange.com', 'drupal.stackexchange.com', 'dsp.stackexchange.com', 'earthscience.stackexchange.com', 'ebooks.stackexchange.com', 'economics.stackexchange.com', 'electronics.stackexchange.com', 'elementaryos.stackexchange.com', 'ell.stackexchange.com', 'emacs.stackexchange.com', 'engineering.stackexchange.com', 'english.stackexchange.com', 'es.stackoverflow.com', 'ethereum.stackexchange.com', 'expatriates.stackexchange.com', 'expressionengine.stackexchange.com', 'fitness.stackexchange.com', 'freelancing.stackexchange.com', 'french.stackexchange.com', 'gamedev.stackexchange.com', 'gaming.stackexchange.com', 'gardening.stackexchange.com', 'genealogy.stackexchange.com', 'german.stackexchange.com', 'gis.stackexchange.com', 'graphicdesign.stackexchange.com', 'ham.stackexchange.com', 'hardwarerecs.stackexchange.com', 'health.stackexchange.com', 'hermeneutics.stackexchange.com', 'hinduism.stackexchange.com', 'history.stackexchange.com', 'homebrew.stackexchange.com', 'hsm.stackexchange.com', 'islam.stackexchange.com', 'italian.stackexchange.com', 'ja.stackoverflow.com', 'japanese.stackexchange.com', 'joomla.stackexchange.com', 'judaism.stackexchange.com', 'korean.stackexchange.com', 'languagelearning.stackexchange.com', 'latin.stackexchange.com', 'law.stackexchange.com', 'lifehacks.stackexchange.com', 'linguistics.stackexchange.com', 'magento.stackexchange.com', 'martialarts.stackexchange.com', 'math.stackexchange.com', 'matheducators.stackexchange.com', 'mathematica.stackexchange.com', 'mathoverflow.net', 'mechanics.stackexchange.com', 'meta.3dprinting.stackexchange.com', 'meta.academia.stackexchange.com', 'meta.ai.stackexchange.com', 'meta.android.stackexchange.com', 'meta.anime.stackexchange.com', 'meta.apple.stackexchange.com', 'meta.arabic.stackexchange.com', 'meta.arduino.stackexchange.com', 'meta.askubuntu.com', 'meta.astronomy.stackexchange.com', 'meta.aviation.stackexchange.com', 'meta.avp.stackexchange.com', 'meta.beer.stackexchange.com', 'meta.bicycles.stackexchange.com', 'meta.biology.stackexchange.com', 'meta.bitcoin.stackexchange.com', 'meta.blender.stackexchange.com', 'meta.boardgames.stackexchange.com', 'meta.bricks.stackexchange.com', 'meta.buddhism.stackexchange.com', 'meta.chemistry.stackexchange.com', 'meta.chess.stackexchange.com', 'meta.chinese.stackexchange.com', 'meta.christianity.stackexchange.com', 'meta.civicrm.stackexchange.com', 'meta.codegolf.stackexchange.com', 'meta.codereview.stackexchange.com', 'meta.coffee.stackexchange.com', 'meta.cogsci.stackexchange.com', 'meta.computergraphics.stackexchange.com', 'meta.cooking.stackexchange.com', 'meta.craftcms.stackexchange.com', 'meta.crafts.stackexchange.com', 'meta.crypto.stackexchange.com', 'meta.cs.stackexchange.com', 'meta.cstheory.stackexchange.com', 'meta.datascience.stackexchange.com', 'meta.dba.stackexchange.com', 'meta.diy.stackexchange.com', 'meta.drupal.stackexchange.com', 'meta.dsp.stackexchange.com', 'meta.earthscience.stackexchange.com', 'meta.ebooks.stackexchange.com', 'meta.economics.stackexchange.com', 'meta.electronics.stackexchange.com', 'meta.elementaryos.stackexchange.com', 'meta.ell.stackexchange.com', 'meta.emacs.stackexchange.com', 'meta.engineering.stackexchange.com', 'meta.english.stackexchange.com', 'meta.es.stackoverflow.com', 'meta.ethereum.stackexchange.com', 'meta.expatriates.stackexchange.com', 'meta.expressionengine.stackexchange.com', 'meta.fitness.stackexchange.com', 'meta.freelancing.stackexchange.com', 'meta.french.stackexchange.com', 'meta.gamedev.stackexchange.com', 'meta.gaming.stackexchange.com', 'meta.gardening.stackexchange.com', 'meta.genealogy.stackexchange.com', 'meta.german.stackexchange.com', 'meta.gis.stackexchange.com', 'meta.graphicdesign.stackexchange.com', 'meta.ham.stackexchange.com', 'meta.hardwarerecs.stackexchange.com', 'meta.health.stackexchange.com', 'meta.hermeneutics.stackexchange.com', 'meta.hinduism.stackexchange.com', 'meta.history.stackexchange.com', 'meta.homebrew.stackexchange.com', 'meta.hsm.stackexchange.com', 'meta.islam.stackexchange.com', 'meta.italian.stackexchange.com', 'meta.ja.stackoverflow.com', 'meta.japanese.stackexchange.com', 'meta.joomla.stackexchange.com', 'meta.judaism.stackexchange.com', 'meta.korean.stackexchange.com', 'meta.languagelearning.stackexchange.com', 'meta.latin.stackexchange.com', 'meta.law.stackexchange.com', 'meta.lifehacks.stackexchange.com', 'meta.linguistics.stackexchange.com', 'meta.magento.stackexchange.com', 'meta.martialarts.stackexchange.com', 'meta.math.stackexchange.com', 'meta.matheducators.stackexchange.com', 'meta.mathematica.stackexchange.com', 'meta.mathoverflow.net', 'meta.mechanics.stackexchange.com', 'meta.moderators.stackexchange.com', 'meta.monero.stackexchange.com', 'meta.money.stackexchange.com', 'meta.movies.stackexchange.com', 'meta.music.stackexchange.com', 'meta.musicfans.stackexchange.com', 'meta.mythology.stackexchange.com', 'meta.networkengineering.stackexchange.com', 'meta.opendata.stackexchange.com', 'meta.opensource.stackexchange.com', 'meta.outdoors.stackexchange.com', 'meta.parenting.stackexchange.com', 'meta.patents.stackexchange.com', 'meta.pets.stackexchange.com', 'meta.philosophy.stackexchange.com', 'meta.photo.stackexchange.com', 'meta.physics.stackexchange.com', 'meta.pm.stackexchange.com', 'meta.poker.stackexchange.com', 'meta.politics.stackexchange.com', 'meta.portuguese.stackexchange.com', 'meta.productivity.stackexchange.com', 'meta.programmers.stackexchange.com', 'meta.pt.stackoverflow.com', 'meta.puzzling.stackexchange.com', 'meta.quant.stackexchange.com', 'meta.raspberrypi.stackexchange.com', 'meta.retrocomputing.stackexchange.com', 'meta.reverseengineering.stackexchange.com', 'meta.robotics.stackexchange.com', 'meta.rpg.stackexchange.com', 'meta.ru.stackoverflow.com', 'meta.rus.stackexchange.com', 'meta.russian.stackexchange.com', 'meta.salesforce.stackexchange.com', 'meta.scicomp.stackexchange.com', 'meta.scifi.stackexchange.com', 'meta.security.stackexchange.com', 'meta.serverfault.com', 'meta.sharepoint.stackexchange.com', 'meta.skeptics.stackexchange.com', 'meta.softwarerecs.stackexchange.com', 'meta.sound.stackexchange.com', 'meta.space.stackexchange.com', 'meta.spanish.stackexchange.com', 'meta.sports.stackexchange.com', 'meta.sqa.stackexchange.com', 'meta.stackexchange.com', 'meta.stackoverflow.com', 'meta.startups.stackexchange.com', 'meta.stats.stackexchange.com', 'meta.superuser.com', 'meta.sustainability.stackexchange.com', 'meta.tex.stackexchange.com', 'meta.tor.stackexchange.com', 'meta.travel.stackexchange.com', 'meta.tridion.stackexchange.com', 'meta.unix.stackexchange.com', 'meta.ux.stackexchange.com', 'meta.vi.stackexchange.com', 'meta.webapps.stackexchange.com', 'meta.webmasters.stackexchange.com', 'meta.windowsphone.stackexchange.com', 'meta.woodworking.stackexchange.com', 'meta.wordpress.stackexchange.com', 'meta.workplace.stackexchange.com', 'meta.worldbuilding.stackexchange.com', 'meta.writers.stackexchange.com', 'moderators.stackexchange.com', 'monero.stackexchange.com', 'money.stackexchange.com', 'movies.stackexchange.com', 'music.stackexchange.com', 'musicfans.stackexchange.com', 'mythology.stackexchange.com', 'networkengineering.stackexchange.com', 'opendata.stackexchange.com', 'opensource.stackexchange.com', 'outdoors.stackexchange.com', 'parenting.stackexchange.com', 'patents.stackexchange.com', 'pets.stackexchange.com', 'philosophy.stackexchange.com', 'photo.stackexchange.com', 'physics.stackexchange.com', 'pm.stackexchange.com', 'poker.stackexchange.com', 'politics.stackexchange.com', 'portuguese.stackexchange.com', 'productivity.stackexchange.com', 'programmers.stackexchange.com', 'pt.stackoverflow.com', 'puzzling.stackexchange.com', 'quant.stackexchange.com', 'raspberrypi.stackexchange.com', 'retrocomputing.stackexchange.com', 'reverseengineering.stackexchange.com', 'robotics.stackexchange.com', 'rpg.stackexchange.com', 'ru.stackoverflow.com', 'rus.stackexchange.com', 'russian.stackexchange.com', 'salesforce.stackexchange.com', 'scicomp.stackexchange.com', 'scifi.stackexchange.com', 'security.stackexchange.com', 'serverfault.com', 'sharepoint.stackexchange.com', 'skeptics.stackexchange.com', 'softwarerecs.stackexchange.com', 'sound.stackexchange.com', 'space.stackexchange.com', 'spanish.stackexchange.com', 'sports.stackexchange.com', 'sqa.stackexchange.com', 'stackapps.com', 'stackoverflow.com', 'startups.stackexchange.com', 'stats.stackexchange.com', 'superuser.com', 'sustainability.stackexchange.com', 'tex.stackexchange.com', 'tor.stackexchange.com', 'travel.stackexchange.com', 'tridion.stackexchange.com', 'unix.stackexchange.com', 'ux.stackexchange.com', 'vi.stackexchange.com', 'webapps.stackexchange.com', 'webmasters.stackexchange.com', 'windowsphone.stackexchange.com', 'woodworking.stackexchange.com', 'wordpress.stackexchange.com', 'workplace.stackexchange.com', 'worldbuilding.stackexchange.com', 'writers.stackexchange.com'] \n",
    "# communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_tags(line):\n",
    "    line = xmltodict.parse(line)\n",
    "    creation_date = line['row']['@CreationDate'].split(\"-\")\n",
    "    yyyy_mm = str(creation_date[0] + \"-\" + creation_date[1])\n",
    "    tags = line[\"row\"][\"@Tags\"].replace(\"><\", \",\").replace(\"<\", \"\").replace(\">\", \"\")\n",
    "    tags = tags.split(\",\")\n",
    "    return map(lambda x: (x, yyyy_mm), tags)\n",
    "\n",
    "def group_by_date(x):\n",
    "    creation_dates = {}\n",
    "    for d in set(x):\n",
    "        creation_dates[d] = list(x).count(d)\n",
    "    return creation_dates\n",
    "    \n",
    "def count_tags(community=\"stats.stackexchange.com\"):\n",
    "    wtf = sc.textFile(\"E:\\\\stackexchange_Extracted\\\\\" + community + \"\\\\Posts.xml\")\n",
    "    tag_count = wtf.filter(lambda x: 'PostTypeId=\"1\"' in x).flatMap(get_tags).groupByKey().mapValues(group_by_date)\n",
    "    return tag_count.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3dprinting.stackexchange.com\n",
      "academia.stackexchange.com\n",
      "ai.stackexchange.com\n",
      "android.stackexchange.com\n",
      "anime.stackexchange.com\n",
      "apple.stackexchange.com\n",
      "arabic.stackexchange.com\n",
      "arduino.stackexchange.com\n",
      "askubuntu.com\n",
      "astronomy.stackexchange.com\n",
      "aviation.stackexchange.com\n",
      "avp.stackexchange.com\n",
      "beer.stackexchange.com\n",
      "bicycles.stackexchange.com\n",
      "biology.stackexchange.com\n",
      "bitcoin.stackexchange.com\n",
      "blender.stackexchange.com\n",
      "boardgames.stackexchange.com\n",
      "bricks.stackexchange.com\n",
      "buddhism.stackexchange.com\n",
      "chemistry.stackexchange.com\n",
      "chess.stackexchange.com\n",
      "chinese.stackexchange.com\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 188.0 failed 1 times, most recent failure: Lost task 1.0 in stage 188.0 (TID 9534, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"D:\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 172, in main\n  File \"D:\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 167, in process\n  File \"D:\\spark-2.0.0-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 2371, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"D:\\spark-2.0.0-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 2371, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"D:\\spark-2.0.0-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 317, in func\n    return f(iterator)\n  File \"D:\\spark-2.0.0-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 1876, in combine\n    merger.mergeValues(iterator)\n  File \"D:\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\shuffle.py\", line 236, in mergeValues\n    for k, v in iterator:\n  File \"<ipython-input-122-133d13c6bd39>\", line 5, in get_tags\nUnicodeEncodeError: 'ascii' codec can't encode characters in position 7-8: ordinal not in range(128)\n\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:390)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1871)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1884)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1897)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1911)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:893)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:358)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:892)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:453)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.GeneratedMethodAccessor24.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:128)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:211)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"D:\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 172, in main\n  File \"D:\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 167, in process\n  File \"D:\\spark-2.0.0-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 2371, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"D:\\spark-2.0.0-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 2371, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"D:\\spark-2.0.0-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 317, in func\n    return f(iterator)\n  File \"D:\\spark-2.0.0-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 1876, in combine\n    merger.mergeValues(iterator)\n  File \"D:\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\shuffle.py\", line 236, in mergeValues\n    for k, v in iterator:\n  File \"<ipython-input-122-133d13c6bd39>\", line 5, in get_tags\nUnicodeEncodeError: 'ascii' codec can't encode characters in position 7-8: ordinal not in range(128)\n\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:390)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-125-e6e985a6b6bd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcommunity\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcommunities\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[0mcommunity\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcommunity\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcount_tags\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommunity\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"tags_count.txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"w\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-122-133d13c6bd39>\u001b[0m in \u001b[0;36mcount_tags\u001b[1;34m(community)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mwtf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"E:\\\\stackexchange_Extracted\\\\\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcommunity\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"\\\\Posts.xml\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mtag_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'PostTypeId=\"1\"'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatMap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_tags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupByKey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapValues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup_by_date\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtag_count\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\spark-2.0.0-bin-hadoop2.7\\python\\pyspark\\rdd.pyc\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    774\u001b[0m         \"\"\"\n\u001b[0;32m    775\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 776\u001b[1;33m             \u001b[0mport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    777\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\py4j-0.10.1-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m         return_value = get_return_value(\n\u001b[1;32m--> 933\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m    934\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\spark-2.0.0-bin-hadoop2.7\\python\\pyspark\\sql\\utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\py4j-0.10.1-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    310\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    311\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 312\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    313\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 188.0 failed 1 times, most recent failure: Lost task 1.0 in stage 188.0 (TID 9534, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"D:\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 172, in main\n  File \"D:\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 167, in process\n  File \"D:\\spark-2.0.0-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 2371, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"D:\\spark-2.0.0-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 2371, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"D:\\spark-2.0.0-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 317, in func\n    return f(iterator)\n  File \"D:\\spark-2.0.0-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 1876, in combine\n    merger.mergeValues(iterator)\n  File \"D:\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\shuffle.py\", line 236, in mergeValues\n    for k, v in iterator:\n  File \"<ipython-input-122-133d13c6bd39>\", line 5, in get_tags\nUnicodeEncodeError: 'ascii' codec can't encode characters in position 7-8: ordinal not in range(128)\n\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:390)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1871)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1884)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1897)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1911)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:893)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:358)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:892)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:453)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.GeneratedMethodAccessor24.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:128)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:211)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"D:\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 172, in main\n  File \"D:\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 167, in process\n  File \"D:\\spark-2.0.0-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 2371, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"D:\\spark-2.0.0-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 2371, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"D:\\spark-2.0.0-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 317, in func\n    return f(iterator)\n  File \"D:\\spark-2.0.0-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 1876, in combine\n    merger.mergeValues(iterator)\n  File \"D:\\spark-2.0.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\shuffle.py\", line 236, in mergeValues\n    for k, v in iterator:\n  File \"<ipython-input-122-133d13c6bd39>\", line 5, in get_tags\nUnicodeEncodeError: 'ascii' codec can't encode characters in position 7-8: ordinal not in range(128)\n\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:390)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "for community in communities:\n",
    "    print community\n",
    "    results[community] = count_tags(community)\n",
    "with open(\"tags_count.txt\", \"w\") as f:\n",
    "    f.write(json.dumps(results))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
